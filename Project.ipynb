{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Recommendation by Using LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSDM - KKBox's Music Recommendation Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a competition released on Kaggle (Link see [3])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we have to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered.\n",
    "\n",
    "If there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. The same rule applies to the testing set.\n",
    "\n",
    "Kaggle provides a training data set consists of information of the first observable listening event for each unique user-song pair within a specific time duration. Metadata of each unique user and song pair is also provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we adopt boosting machine learning technique.\n",
    "\n",
    "The philosophy of boosting is to use a set of __weak learnners__ to create a __strong learner__. A __weak learner__ is defined to be a classifier which is only slightly correlated with the true classification. In contrast, a __strong learner__ is a classifier that is arbitrarily well-correlated with the true classification.\n",
    "\n",
    "Boosting algorithms consist of iteratively training weak learners with respect to a distribution and adding them to a final strong learner. When the weak learners are added, they are typically weighted with respect to their accuracy. Every time after a weak learner is added, the data are reweighted: __examples that are misclassified gain weight and examples that are classified correctly lose weight__. In this way, future weak learners will focus more on the examples that previous weak learners misclassified. Then by adding new weak learners again and again, finally we will get a strong learner that could handle the task much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Gradient Boosting Machine (LightGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the weak learner is chosen as (small fixed size) __decision tree__, and differentiable loss functions are adopted, we get a __Gradient Boosting Decision Tree (GBDT)__ [1]. Due to its efficiency, accuracy, and interpretability, GBDT achieves state-of-the-art performances in many machine learning tasks, such as multi-class classification, click prediction, and learning to rank.\n",
    "\n",
    "However, __conventional implementations__ of GBDT need to, for every feature, scan all the data instances to estimate the information gain of all the possible split points. This makes these implementations very time consuming when handling big data. To this end, in 2017 NIPS, Microsoft published a __new implementation__ of GBDT, called __LightGBM__, which speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy in experiments on multiple public datasets [1].\n",
    "\n",
    "LightGBM adopted two novel techiques to try to achieve better performance:\n",
    "\n",
    "1. Gradient-based One-Side Sampling (GOSS). According to the definition of information gain, instances with larger gradients (under-trained instances) will contribute more to the information gain. So when down sampling the data instances, one could better keep those instances with large gradients, and only randomly drop those instances with small gradients. It is proved that such a treatment can lead to a more accurate gain estimation than uniformly random sampling, with the same target sampling rate.\n",
    "\n",
    "2. Exclusive Feature Bundling (EFB). In real applications, usually the feature space is quite sparse, many features are (almost) exclusive, i.e., they rarely take nonzero values simultaneously. So it is safely to bundle such exclusive features. To this end, LightGBM includes an efficient algorithm by reducing the optimal bundling problem to a graph coloring problem, and solving it by a greedy algorithm with a constant approximation ratio.\n",
    "\n",
    "With the reduction of size of training data and reduction of number of features, LightGBM is equipped with the ability of faster training speed and higher efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microsoft made its LightGBM package open source. The package can be found and installed from Github [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our code is based on an anonymous author in the discussion area of Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original training dataset provided by Kaggle is too large for our slow computers to work on. So does the test dataset. Furthermore, the test data provided by Kaggle does not have labels. This makes us unable to see how the algorithm works. Therefore, we decide to discard the test data. Instead, we extract 10,000 samples from the training data, and split it into a larger group with 7000 samples for training and a smaller group with 3000 samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "ntr = 7000\n",
    "nts = 3000\n",
    "print('Loading data...')\n",
    "data_path = './data/'\n",
    "train = pd.read_csv(data_path + 'train.csv',nrows=ntr)\n",
    "names=['msno','song_id','source_system_tab','source_screen_name',\\\n",
    "      'source_type','target']\n",
    "test1 = pd.read_csv(data_path+'train.csv',names=names,skiprows=ntr,nrows=nts)\n",
    "songs = pd.read_csv(data_path + 'songs.csv')\n",
    "members = pd.read_csv(data_path + 'members.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the learned recommendation system in the end, we extract the true labels of test data into ytr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test1.drop(['target'],axis=1)\n",
    "ytr = np.array(test1['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rearrange columns of the test data so that it fits into remain codes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_name = ['id','msno','song_id','source_system_tab',\\\n",
    "             'source_screen_name','source_type']\n",
    "test['id']=np.arange(nts)\n",
    "test = test[test_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing...\n"
     ]
    }
   ],
   "source": [
    "print('Data preprocessing...')\n",
    "song_cols = ['song_id', 'artist_name', 'genre_ids', 'song_length', 'language']\n",
    "train = train.merge(songs[song_cols], on='song_id', how='left')\n",
    "test = test.merge(songs[song_cols], on='song_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "members['registration_year'] = members['registration_init_time'].apply(lambda x: int(str(x)[0:4]))\n",
    "members['registration_month'] = members['registration_init_time'].apply(lambda x: int(str(x)[4:6]))\n",
    "members['registration_date'] = members['registration_init_time'].apply(lambda x: int(str(x)[6:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "members['expiration_year'] = members['expiration_date'].apply(lambda x: int(str(x)[0:4]))\n",
    "members['expiration_month'] = members['expiration_date'].apply(lambda x: int(str(x)[4:6]))\n",
    "members['expiration_date'] = members['expiration_date'].apply(lambda x: int(str(x)[6:8]))\n",
    "members = members.drop(['registration_init_time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "members_cols = members.columns\n",
    "train = train.merge(members[members_cols], on='msno', how='left')\n",
    "test = test.merge(members[members_cols], on='msno', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del members, songs; gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = list(train.columns)\n",
    "cols.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 48.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(cols):\n",
    "    if train[col].dtype == 'object':\n",
    "        train[col] = train[col].apply(str)\n",
    "        test[col] = test[col].apply(str)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        train_vals = list(train[col].unique())\n",
    "        test_vals = list(test[col].unique())\n",
    "        le.fit(train_vals + test_vals)\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col] = le.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Song popularity\n",
    "unique_songs = range(max(train['song_id'].max(), test['song_id'].max()))\n",
    "song_popularity = pd.DataFrame({'song_id': unique_songs, 'popularity':0})\n",
    "\n",
    "train_sorted = train.sort_values('song_id')\n",
    "train_sorted.reset_index(drop=True, inplace=True)\n",
    "test_sorted = test.sort_values('song_id')\n",
    "test_sorted.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 5770/5770 [00:24<00:00, 235.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for unique_song in tqdm(unique_songs):\n",
    "    if unique_song != (len(unique_songs)-1):\n",
    "        train_pop = (train_sorted['song_id'].searchsorted(unique_song+1)[0] - \n",
    "                     train_sorted['song_id'].searchsorted(unique_song)[0])\n",
    "        test_pop = (test_sorted['song_id'].searchsorted(unique_song+1)[0] - \n",
    "                     test_sorted['song_id'].searchsorted(unique_song)[0])\n",
    "    else : \n",
    "        train_pop = (len(train_sorted) -\n",
    "                     train_sorted['song_id'].searchsorted(unique_song)[0])\n",
    "        test_pop = (len(test_sorted) -\n",
    "                     test_sorted['song_id'].searchsorted(unique_song)[0])\n",
    "    song_popularity[unique_song] = train_pop + test_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# User library size\n",
    "\n",
    "\n",
    "X = np.array(train.drop(['target'], axis=1))\n",
    "y = train['target'].values\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "ids = test['id'].values\n",
    "\n",
    "del train, test; gc.collect();\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \\\n",
    "    test_size=0.1, random_state = 12)\n",
    "    \n",
    "del X, y; gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "d_valid = lgb.Dataset(X_valid, label=y_valid) \n",
    "\n",
    "watchlist = [d_train, d_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\ttraining's auc: 0.990018\tvalid_1's auc: 0.844558\n",
      "[20]\ttraining's auc: 0.999901\tvalid_1's auc: 0.839645\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's auc: 0.997805\tvalid_1's auc: 0.849754\n"
     ]
    }
   ],
   "source": [
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.4\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 15\n",
    "params['num_leaves'] = 2**8\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'auc'\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=200, valid_sets=watchlist, \\\n",
    "early_stopping_rounds=10, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions and saving them...\n"
     ]
    }
   ],
   "source": [
    "print('Making predictions and saving them...')\n",
    "p_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submission.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each id in the test data, the model predicted the probablity that the user will listen to the corresponding song for the second time in the future. The result is stored in p_test. Now We use a hard thredhold rule to obtain yhat. That is, if $p_{test}[i]>0.5$, $yhat=1$, else $yhat=0$. Then we can calculate the accuracy acc of our lgbm model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of lgbm model on test data is: 77.900000%\n"
     ]
    }
   ],
   "source": [
    "yhat = (p_test>0.5).astype(int)\n",
    "comp = (yhat==ytr).astype(int)\n",
    "acc = comp.sum()/comp.size*100\n",
    "print('The accuracy of lgbm model on test data is: {0:f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a comparison, we use a random guessing to predict on the same test data set. That is, to assign yhat_rand 1 or 2 for each id in test data according to $Bernoulli(1/2)$ distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of random model on test data is: 49.933333%\n"
     ]
    }
   ],
   "source": [
    "rd_seed = np.random.uniform(0,1,nts)\n",
    "yhat_rand = (rd_seed>0.5).astype(int)\n",
    "comp_rand = (yhat_rand==ytr).astype(int)\n",
    "acc_rand = comp_rand.sum()/comp_rand.size*100\n",
    "print('The accuracy of random model on test data is: {0:f}%'.format(acc_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, lgbm model is better than random guessing. This means that the chosen lgbm model indeed improved the predicition accuracy on this problem. Of course, it is just a first trial, there are much work to do if one wants to further enhance the prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Friedman J H. Greedy function approximation: a gradient boosting machine[J]. Annals of statistics, 2001: 1189-1232."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] Ke G, Meng Q, Wang T, et al. A Highly Efficient Gradient Boosting Decision Tree[C]//Advances in Neural Information Processing Systems. 2017: 3148-3156."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] https://www.kaggle.com/c/kkbox-music-recommendation-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] https://github.com/Microsoft/LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
